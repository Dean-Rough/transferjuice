#!/usr/bin/env tsx
/**
 * Test Playwright Twitter Scraper
 * Reliable scraping with media support
 */

import { config } from 'dotenv';
import { getPlaywrightScraper } from '@/lib/twitter/playwright-scraper';
import { GLOBAL_ITK_SOURCES } from '@/lib/twitter/globalSources';

// Load environment variables
config({ path: '.env.local' });

async function testPlaywrightScraper() {
  console.log('ðŸŽ­ Testing Playwright Twitter Scraper\n');
  
  const scraper = getPlaywrightScraper({
    headless: false, // Show browser for testing
    timeout: 30000,
  });
  
  try {
    // Test connection first
    console.log('ðŸ”— Testing Twitter connection...');
    const isConnected = await scraper.testConnection();
    
    if (!isConnected) {
      console.error('âŒ Cannot connect to Twitter. Check your internet connection.');
      return;
    }
    
    console.log('âœ… Twitter is accessible\n');
    
    // Test with a few ITK sources
    const testSources = [
      { username: 'FabrizioRomano', name: 'Fabrizio Romano' },
      { username: 'David_Ornstein', name: 'David Ornstein' },
      { username: 'JBurtTelegraph', name: 'Jason Burt' }
    ];
    
    for (const source of testSources) {
      console.log(`\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`);
      console.log(`ðŸ“° Scraping @${source.username} - ${source.name}`);
      console.log(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n`);
      
      try {
        const startTime = Date.now();
        const tweets = await scraper.scrapeUserTweets(source.username, 5);
        const elapsed = Date.now() - startTime;
        
        console.log(`â±ï¸ Scraped ${tweets.length} tweets in ${elapsed}ms\n`);
        
        for (const tweet of tweets) {
          console.log(`ðŸ“ Tweet ID: ${tweet.id}`);
          console.log(`   Text: "${tweet.text.substring(0, 100)}${tweet.text.length > 100 ? '...' : ''}"`);
          console.log(`   Posted: ${tweet.createdAt.toLocaleString()}`);
          console.log(`   Author: @${tweet.author.username} ${tweet.author.verified ? 'âœ“' : ''}`);
          
          if (tweet.media.length > 0) {
            console.log(`   ðŸ“· Media: ${tweet.media.length} attachment(s)`);
            for (const media of tweet.media) {
              console.log(`      - ${media.type}: ${media.url.substring(0, 50)}...`);
            }
          }
          
          console.log(`   ðŸ’¬ Replies: ${tweet.replies}\n`);
        }
        
        // Highlight transfer-related content
        const transferKeywords = ['transfer', 'signing', 'medical', 'fee', 'contract', 'deal'];
        const transferTweets = tweets.filter(t => 
          transferKeywords.some(keyword => t.text.toLowerCase().includes(keyword))
        );
        
        if (transferTweets.length > 0) {
          console.log(`ðŸ”¥ Found ${transferTweets.length} potential transfer tweet(s)`);
        }
        
      } catch (error) {
        console.error(`âŒ Failed to scrape @${source.username}:`, error);
      }
      
      // Small delay between users
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
    
    // Test batch scraping
    console.log('\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
    console.log('ðŸ“¦ Testing Batch Scraping');
    console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
    
    const batchUsernames = ['SkySportsNews', 'BBCSport', 'TheAthleticFC'];
    console.log(`Scraping ${batchUsernames.length} accounts: ${batchUsernames.join(', ')}\n`);
    
    const batchStart = Date.now();
    const results = await scraper.scrapeMultipleUsers(batchUsernames, 3);
    const batchElapsed = Date.now() - batchStart;
    
    console.log(`âœ… Batch complete in ${Math.round(batchElapsed / 1000)}s\n`);
    
    for (const [username, tweets] of results) {
      console.log(`@${username}: ${tweets.length} tweets scraped`);
    }
    
  } catch (error) {
    console.error('Fatal error:', error);
  } finally {
    // Clean up
    await scraper.close();
  }
  
  console.log('\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
  console.log('ðŸ’¡ Playwright Scraper Summary');
  console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n');
  
  console.log('Pros:');
  console.log('âœ… Works reliably without rate limits');
  console.log('âœ… Gets all media (images, videos)');
  console.log('âœ… Handles dynamic content');
  console.log('âœ… Can bypass most anti-scraping measures\n');
  
  console.log('Cons:');
  console.log('âš ï¸ Slower than API (3-5s per account)');
  console.log('âš ï¸ Requires more resources');
  console.log('âš ï¸ May need proxy rotation for scale\n');
  
  console.log('For production, consider:');
  console.log('â€¢ Running headless for better performance');
  console.log('â€¢ Implementing proxy rotation');
  console.log('â€¢ Adding retry logic for failed pages');
  console.log('â€¢ Caching results aggressively\n');
}

// Run the test
testPlaywrightScraper().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});